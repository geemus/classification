dynamodb operations (especially delete/put) should be queue/worker based

post /categories/category
  batch_get_item in lookups should paginate if unproccessed items is non-empty
  conditional should use total tokens in all categories (separate table?)
  weighted should use total tokens in this category

separate logic and sinatra/api stuff (to simplify testing, if nothing else...)

change to development/test/etc databases (ie classification:development:good) or something
  (note that shorter is probably faster, so perhaps some kind of shorthand, ie c:d:good)

=====

Beyond rss feeds... activity/status feeds (facebook, github, twitter)

=====

persistence - postgres
  postgres - http://stackoverflow.com/questions/4683057/simple-example-of-postgres-query-in-ruby
  setup
    DROP TABLE IF EXISTS tokens;
    CREATE TABLE tokens (name TEXT PRIMARY KEY, count INTEGER DEFAULT 0);

  upserting tokens
    use PREPARE for effeciency (only lasts as long as session though, so must redo each time)
    keep everything in a single BEGIN; COMMIT; block to avoid committing after each request
    ? redefining function seems bad, but not sure how else to do it since PREPARES are transient
================================================================================
BEGIN;

PREPARE insert_token (TEXT, INTEGER) AS INSERT INTO tokens (name, count) VALUES ($1, $2);
PREPARE update_token (TEXT, INTEGER) AS UPDATE tokens SET count = count + $2 WHERE name = $1;

CREATE OR REPLACE FUNCTION upsert(name TEXT, count INTEGER) RETURNS VOID AS
$$
BEGIN
  LOOP
    -- attempt update
    EXECUTE update_token(name, count);
    IF found THEN
    RETURN;
    END IF;
    -- if not attempt to insert
    BEGIN
      EXECUTE insert_token(name, count);
      RETURN;
    EXCEPTION when unique_violation THEN
      -- do nothing and loop to retry update
    END;
  END LOOP;
END;
$$
LANGUAGE plpgsql;

-- call upsert once for each token to update

COMMIT;
================================================================================

    CREATE FUNCTION merge_db(key INT, data TEXT) RETURNS VOID AS
    $$
    BEGIN
    LOOP
    -- first try to update the key
    UPDATE db SET b = data WHERE a = key;
    IF found THEN
    RETURN;
    END IF;
    -- not there, so try to insert the key
    -- if someone else inserts the same key concurrently,
    -- we could get a unique-key failure
    BEGIN
    INSERT INTO db(a,b) VALUES (key, data);
    RETURN;
    EXCEPTION WHEN unique_violation THEN
    -- Do nothing, and loop to try the UPDATE again.
    END;
    END LOOP;
    END;
    $$
    LANGUAGE plpgsql;

    SELECT merge_db(1, 'david');
    use prepare once and multiple executes to avoid reparsing/replanning
      PREPARE fooplan (int, text, bool, numeric) AS
        INSERT INTO foo VALUES($1, $2, $3, $4);
        EXECUTE fooplan(1, 'Hunter Valley', 't', 200.00);

    wrap all operations in one begin/commit to avoid multiple commits
      BEGIN;
      -- other operations
      SAVEPOINT sp1;
      INSERT INTO wines VALUES('Chateau Lafite 2003', '24');
      -- Assume the above fails because of a unique key violation,
      -- so now we issue these commands:
      ROLLBACK TO sp1;
      UPDATE wines SET stock = stock + 24 WHERE winename = 'Chateau Lafite 2003';
      -- continue with other operations, and eventually
      COMMIT;

design web api
implement web api
? use fisher also/instead

tokenization
  ? urls: break out host

reading
  http://www.paulgraham.com/naivebayes.html
  http://www.paulgraham.com/spam.html
  http://paulgraham.com/better.html

reader
  postgres to store subscriptions and meta data, s3 to store blobs, bayes service to store tokens
